{
  "title": "A Human-centric Approach to Fairness in AI",
  "date": "2021-09-16T00:00:00.000Z",
  "tags": [
    "fairness",
    "ethics",
    "veritas",
    "philosophy",
    "responsible-ai",
    "data science",
    "singapore"
  ],
  "lastmod": "2021-12-24T00:00:00.000Z",
  "draft": false,
  "summary": "Fairness is messy and complicated. Attempts to distil it down to a single metric is unhelpful and counter-productive. As business owners and model developers we should embrace the struggle in trying to apply fairness in artificial intelligence and data analytics models.",
  "images": [
    "/static/img/fairness/human-centric-approach-to-ai-cover.png"
  ],
  "layout": "PostLayout",
  "bibliography": "responsible-ai.bib",
  "body": {
    "raw": "\n<TOCInline toc={props.toc} asDisclosure toHeading={3} />\n\n## Background\n\nAfter 4+ years implementing data science solutions, I find myself confronting a familiar set of challenges in which I started my career in - policy evaluation. While this is in the sphere of AI fairness, there are many similar parallels across both domains.\n\nThis post is written as part of our participation in the Monetary Authority of Singapore (MAS) Veritas challenge on applying fairness, ethics, accountability and fairness (FEAT) in artificial intelligence and data analytics (AIDA) use cases.^[For more information, please refer to the [challenge press release](https://www.mas.gov.sg/news/media-releases/2021/mas-launches-global-challenge-to-accelerate-innovation-in-responsible-ai-solutions) and the [recommended assessment methodology](https://www.mas.gov.sg/-/media/MAS/News/Media-Releases/2021/Veritas-Document-1-FEAT-Fairness-Principles-Assessment-Methodology.pdf).]\n\nAs one of the shortlisted finalist, I thought it would be good to blog about the approach we are taking and why we believe it to be the right one. In the initial written proposal, rather than simply pitching our existing experience and expertise, I decided to write an aspirational proposal of what fairness in machine learning could look like, which I feel is very different from what it currently is in practice. I thank the MAS and organising committee for giving us the opportunity to shape some of the most important policy guidelines and recommendations in the space.\n\nOur proposal is novel in a few ways:\n\n1. It is an **open-source solution**. We are transparent by design. It strikes me as odd that methods to evaluate whether a model is fair or not is shrouded in secrecy by commercial companies.^[There are other open-source solutions as well and I will address the differences in the next post, but this post should give a flavour of what we are going to do differently.]\n\n2. We do **not** make ethical judgements of what is fair. We believe that in the issue of fairness there is no one metric to rule them all. Rather a deliberative and consultative process is essential. Though having made a decision, we can make subsequent workstream processes as easy as possible.\n\n3. We embed the evaluation of AI fairness within the **best practices** of machine learning development and operations such as version control, unit testing and continuous integration.\n\nIt has been a fun few weeks exploring the latest thinking of scholars in this field and listening to the masterclass sessions provided by the industry partners. In the spirit of openness, I decided to document my thought process in designing the described framework. This is the first post in a series of three.\n\nFor the first post, I will focus on the issue of fairness and argue that the right approach that should be adopted by the industry is a human-centric one. Attempts to distil fairness down to a single metric are unhelpful and counter-productive. As business owners and model developers we should embrace the struggle in trying to define fairness in relation to the problem we are trying to solve, the models that we are using, and the broader societal context, rather than shun it.\n\n## The case for a single fairness metric\n\nHaving a single metric to quantify what is fair or not seems to be a goal for many AI scholars in the field, as well as many of industry practitioners. After all, while methods might vary, the foundation of machine learning hinges on maximising or minimising clear objectives^[e.g. accuracy or area under ROC curve for a binary classifier or root mean square error for a continuous outcome]. A business problem can then be translated to a mathematical / statistical / computational problem and different methods can be compared against each other based on how far the predicted output differs from the actual output as measured by the objective function.\n\nIf fairness could be distilled down to a single metric, a data scientist can include it as part of the objective function or as a constraint and find an ideal point that maximises overall business needs while satisfying fairness requirements.\n\n## A brief survey of fairness metrics\n\n@barocas-hardt-narayanan lists 19 demographic fairness criteria for classification problems. This includes measures such as _Demographic Parity / Statistical Parity_ [@dwork2012], _Equalized Odds Metric_ [@hardt-price-srebo] and _Calibration within Groups_ [@chouldechova2017]. They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence.\n\nIn another survey of fairness definitions, @verma-rubin listed 20 definitions of fairness, 13 belonging to statistical measures, 3 being classified as similarity-based measures and the remaining 4 stemming from causal reasoning. Applying a logistic regression to a German credit dataset, they showed that approximately half of their definitions are satisfied while the other half are not satisfied:\n\n![fairness-metrics-verma-rubin](/static/img/fairness/verma-table.png)\n\nConcluding their assessment of their analysis, they wrote:\n\n> So, is the classifier fair? Clearly, the answer to this question depends on the notion of fairness one wants to adopt. We believe more work is needed to clarify which definitions are appropriate to each particular situation.\n\n@saxena2019, and @srivastava2019 conducted experiments to find out how public attitudes map to fairness definitions for machine learning. The first paper found calibrated fairness tends to be preferred in the context of loan decisions while the second study found that demographic parity most closely matches people's idea of fairness as applied to criminal risk prediction and skin cancer risk prediction.\n\n![srivastava-fairness-ui](/static/img/fairness/srivastava-fairness-question.png)\n\nWhile the intentions of the research were right and is an interesting area to explore, the methodology is lacking. Both papers ask respondents sourced through Amazon's mechanical turk to answer a series of questions. As the questions very explicitly mentioned race and gender attributes, respondents are primed to perceive fairness through the lens of a redistributive one. The most one could claim is that if subjects are forced to reason about fairness in a utilitarian cost-benefit setting, they tend to prefer the selected metrics.\n\nThe right approach I argue would be to take a more holistic view of what's fair and ethical, before re-introducing the issue of what metrics. In the subsequent sections, I highlight the limitations with the existing approach by pointing out three facts and explore their implications:\n\n1. Data as a reflection of ourselves and society\n2. Models as tools for discrimination\n3. The difficulty of agreeing on a common definition of what's ethical\n\n## Data as mirrors\n\nIn a discussion on automated hiring systems and biases^[The word bias in this post should be understood in the social justice sense and not as the statistical concept] in the hiring process, @stoyanovich-howe-jagadish eloquently warns about the inherent biases of a dataset using an analogy of data as a mirrored reflection of the world:\n\n> Informally, data is a mirror reflection of the world. More often than not, this reflection is distorted. One reason for this may be that the mirror itself (the measurement process) is distorted: it faithfully reflects some portions of the world, while amplifying or diminishing others. Another reason may be that even if the mirror was perfect, it may be reflecting a distorted world â€” a world such as it is, and not as it could or should be.\n\nA data-centric approach is inherently limited, as it lacks information about the context in which it is generated, processed and analysed. Just like how a reflection does not know it is distorted, or why it is distorted, we should not expect the data to know information about itself. While some might think collecting more data might alleviate data issues, the size of the data can be regarded as the size of the mirror - a larger mirror does not mean that the image is less distorted.\n\nSimilarly, we cannot expect an algorithm to make a judgement of what is fair or not. While, I shall argue in the subsequent sections that even if the algorithm has complete information about the context in which the data is generated it should still not be able to make such a judgement, the fact that it lacks such historical context and understanding should already be red flags for anyone trying to think that it could.\n\nA dataset forces a particular perspective of reality. It does not tell us what is not captured or other possible alternatives. This is not to say that the reflection observed is not important - it is, but we need to bring in additional perspectives and dimensions to contextualise our understanding.\n\n## Models as discriminatory\n\nEvery model is discriminatory.^[Discriminatory is used over here in the sense of being able to differentiate, not in the social or legal sense of the word.] In the process of trying to optimise against a particular objective function, it tries its best to discriminate against all possible dimensions available in a dataset. This applies to complicated deep neural networks, simpler regression trees and even heuristic models.\n\nIn the context of a hiring decision, the model might make a classification based on input features such as test scores, psychometric scores, interviewer ratings and of course age, gender and race. The ability of more complicated models to discriminate is the main rationale for the use of more powerful and deeper machine learning models.\n\nSometimes it discriminates too much and is not effective in real world applications - i.e. the model has overfitted the dataset. Hence, the use of a hold-out test set to cross-validate the results and ensure it is able to generalize more broadly in world world cases. Nonetheless, left unconstrained, a model will pursue the single goal it is created for, discriminate against all attributes to maximise an objective function, including attributes such as race, gender or religion.\n\nEven if such attributes are omitted, the model would also be able to learn existing attributes in a dataset and its correlation with the protected attributes. @barocas-hardt-narayanan gives an example of how browsing data might be used differentiate the sexes. While each website provides a small signal, their joint distribution can strongly predict whether a user is male or female.\n\nIf models are inherently discriminatory, it is up to humans to decide what should be discriminated against and what is not.^[I think there's no lack of a human's ability to discriminate as well, we do it every day. There just seems to be a fear when it comes to making such discriminations explicit.] Relying on a single utility function to determine the tradeoffs between fairness and efficiency seems to be a way of evading responsibility and accountability.\n\n## Ethics and Fairness\n\nThe debate on ethics, fairness and justice is not new and has been contested by philosophers and political scientists from Plato to Rawls. I will probably not do justice in summarising the nuances in the debate, but I think it's still worth highlighting a few different perspectives to show how standards of what is fair varies.\n\nBefore we can dive into the question of what is fair or equitable, we have to ask \"fair\" by what values or standards?^[Could something be fair but unethical or unfair but ethical? It lies very much in the definition of the terms, but here are examples of each. One could argue that as a system for justice \"an eye for an eye\" is fair but unethical. On the other hand, treating people with different vaccination statuses differently would probably be unfair but ethical.] The issue of what is right or wrong brings us into the heart of ethics.\n\n### Utilitarianism\n\nOne way that we could determine whether a particular act is right would be by summing up the benefits and substracting the cost of its associated harms. If the sum of such a measure (utility) across all affected individuals is positive, it is ethically correct.\n\nIntuitively, this definition of **act utilitarianism** maps most closely to what is possible by the single fairness metric formulation. If we could list out the benefits and harms of every outcome of a models decision, we could create a single utility function that maximises that objective, and in the process of doing so, we maximise social utility.^[In the case of a binary classifier, we can assign a score to each potential outcome (true positive, false positive, true negative and false negative) and use the scores as part of the objective function.]\n\nAlternatively, we could take a step back and ask what would happen if every person / company were to adopt such a rule or policy, would it be in the best interest of society? This line of reasoning, also known as **rule utilitarianism** seeks to ground the mathematical formulation of act utilitarianism with some sense of check and balances.\n\nWhile possibly easier to implement as it relates it AI systems, utilitarianism has major gaps, both practically and morally. Practically, it is nigh impossible to measure benefits and harms across disparate areas (e.g. health vs wealth), or agree on a suitable discount factor to consider the longer lasting effects of policies. Morally, it is hard to be convinced that we should only be concerning ourselves with the consequences of an action and not the action itself. It is also easy to imagine a scenario where a model can be deployed where the benefits to the majority outweigh the harm to a minority class. Morally such as an action would probably strike us as unfair but it would be ethical in a utilitarian framework.\n\n### Deontological Ethics\n\nIn a utilitarian framework, every consequence can be measured and compared, including the value of a human life. This might strike us as unsettling, not because it is hard to put a price tag to life, but just because we think that human life should not be valued and compared. For an AI self-driving car, when faced with a decision of serving to save a life, it should not be calculating the resulting damage and weighing it against the value of a human life. Instead, it should serve, just because saving a life is the right thing to do.\n\nSuch a way of thinking falls squarely in the realm of duty ethics or deontology. Some choices cannot be justified by their effects, no matter how good their consequences are - right takes priority over good.\n\nWhile there are variations within the deontological ethics community such, whether to focus on an agent's duties or a victim's rights, they share in common a belief as espoused by @kant1785 that the only thing unqualifiedly good is a good will.^[Differing definitions on what a good will is gives us different Kantian schools of thought.]\n\nFor the rest of human decision making that does not classify as moral decisions, there is no categorical imperative to do anything associated about it.^[It might be morally praiseworthy but that's no categorical ought.] This seems like a more sane basis to ground our decision making process on and seems to be more aligned with moral intuitions of society in general. Practically, this translates to a different way of evaluating models - if the system does not make decisions that contradict what we believe as categorical imperatives then there is no reason not to give the machine the flexibility to optimize.\n\n### Virtues\n\nInstead of thinking about what goodness and ethics might mean for a machine, we can ask what are the characteristics of a good person. Through that lens, we might say that a person is ethical if he lives by certain traits or virtues which one might regard as morally good.\n\nThe apparent simplicity and practicality of such a system of morality are also its greatest strengths. @foot1997 in Virtue and vices, lists three essential feature of a virtue:\n\n1. It's a disposition of the will\n2. It's beneficial to others, or to its possessor as well as to others\n3. It is corrective of some bad general human tendency\n\nThis list might correspond to virtues like Aristotle's Nicomachean Ethics which includes values such as Courage, Magnanimity and Truthfulness, or it could correspond to theological values of the church - Faith, Hope and Charity.\n\nIn this age of technology and automation, @vallor2016 argues that virtue ethics can be a shining beacon that guides decision makers in techno-sociological choices as it is \"ideally suited for adaptation to the open-ended and varied encounters with particular technologies that will shape the human condition in this and coming centuries\". While agreeing on a common standard of shared virtue ethic might be a herculean task, it is not unreasonable to expect companies and individuals to have a set of core values that they believe in, and would form the basis of evaluating the impact of AIDA models.^[The list of values proposed by Vallor could be a good start: honesty, self-control, humility, justice, courage, empathy, care, civility, flexibility, perspective, magnanimity, and technomoral wisdom.]\n\nOne might ask, how would it be possible to embed moral virtues in an artificial intelligence system and question whether that can be operationalised. Rather than expecting a system to learn a set of values, I think it makes more sense to ask whether the outcomes produced by these systems conform to our internal values. Instead of asking whether a gun is ethical, we should ask in what context and what circumstances is the use of a gun ethical.\n\nOperationally, instead of calculating the expected utility of benefits and harms, these benefits and harms could be presented to an internal review board and accessed based on a set of virtues. If the organisation has virtues such as justice, empathy or care on their checklist, they would be able to flag out a scenario where the majority benefits at the expense of a minority group. If such values are not present, we can then turn the discussion towards the values of an organisation, rather than to talk about AI values which is a pointless exercise.\n\n### Practical Ethics\n\nThe discussion above on three popular ethical schools of thought is not meant to promote one over the other. Practically, human beings have a deeply complex value and judgement system which might not even be internally consistent. In certain cases, one might adopt a utilitarian point of view, in other situations, we might choose to adhere to certain religious principles and beliefs.\n\nOne thing it does show is that what is ethical or not can be highly contentious and we should not restrict ourselves to a single metric in trying to categorise what is right or wrong. Not only is it incapable of capturing the crux of ethical arguments put forth in such debates, but it also artificially restricts us to a set of measures that might not map to what society perceives as ethical.\n\nNext, we turn our attention to the issue of fairness since it is often brought up in discussions on AI governance. We have already seen it mentioned in the above section on virtue ethics and one might claim that we ought to be fair in the application of AI systems, but what do we exactly mean by fairness?^[In a utilitarian framework, fairness could simply arise if we assume individuals have a utility function which exhibits diminishing marginal utility.]\n\n### Fairness\n\nAristotle's _principle of equality_ states that:\n\n> equals should be treated equally and unequals unequally.\n\nThe quote suggests that individuals should be treated the same, unless they differ in ways that are relevant to the situation in which they are involved. For example, if both Jack and Jill can do the work to the same standard, they should have the same chance of being hired. If Jack has a higher chance of being hired just by being a man, we can say that it is a case of unfair hiring.\n\nHowever, the principle of equality does not take into account individual differences that play a part in explaining one's circumstances. In the context of hiring, the opportunities available to women might vary significantly from those available to a man. These differences accumulate over time and translate to differences in work output, even if both men and women were initially equally capable and talented at birth. One might argue that fairness, in this case, is equity - everyone should have equal access to the same opportunities.^[E.g. Affirmative action policies and wealth redistribution policies are examples of how a society might try to mitigate existing unfair biases. To what extent are the policies sufficient to achieve equity is another question.]\n\n## Fairness in AI systems\n\nIn the context of AI decision making systems, concepts of equality or equity deserve additional scrutiny. The metrics that are used to determine what is fair or not, are statistical measures that are derived based on the observed and collected data. Despite the seemingly numerous ways in which one can derive such metrics of fairness, these measures take the state of the world as given, rather than as constructed.^[An exception is the counterfactual school of thought which requires constructing a causal graph which describes the relationships between particular variables, observed or unobserved, and mapping a definition of fairness based on hypothesised pathways and relations.]\n\nIf that is the case, it would be better to name these measures as \"equivalence metrics\" rather than \"fairness metrics\" as they do not map to an ethical sense of fairness. We can ask whether the number of males and females recommended by our AI model is the same or if the estimated false positive or false negative rates are equivalent, but not whether the model is actually _fair_. Fairness requires situating the equivalence metric in the context that it is being applied, understanding historical or structural factors that could possible explain the differences and a value judgement based on some ethical reasoning.\n\nAnother benefit of using the term equivalence rather than fairness is that many of these measures are simply incompatible. Degenerate solutions notwithstanding, mathematically, as formulated in a classification problem, one cannot have equivalence across classes in the number of predicted positives as well as equivalence in the false positive or false negative rates.^[See @barocas-hardt-narayanan for a proof of independence versus separation.] More broadly, how we think about fairness varies across scenarios, but given a specific problem, we can probably articulate why we think a particular distribution or outcome is fair.^[Note, this refers to one outcome, not 20 different fairness outcomes.]\n\nA lot of what is discussed in the machine learning literature touches on fairness (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts fairness to the notion of equality. Of course, we should think about fairness in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud model, it is hard to think that we are truly fair by forcing models to obtain an equal number of fraud cases across certain demographic attributes.\n\nPractically, we should be flipping the approach. One should first ask what fairness means in a given scenario before mapping it to a numerical measure, if applicable. In the case of hiring decisions, where AI is used as a pre-selection step, we might think shortlisting an equal proportion of males and females is fair. While in the case of fraud prevention models, fairness probably means getting the best result for each subgroup by reducing the false positive rate for each group as low as possible and not artificially trying to make them equal!\n\n## Conclusion\n\nIn this post, I question the rationale of fairness metrics as presented in the AI and machine learning literature. I argue that given the nature of data as (distorted) reflections of society, of models as unthinking agents of discrimination (in the broadest sense of the word), we should evaluate such systems from a human-centric perspective.\n\nWhile there is much debate within ethics on what constitutes right and wrong, by using it as a starting point of our decision-making process, we re-introduce human decision making to a process that should be governed by humans. Instead of absconding from our human responsibility on making informed decisions and leaving it to conflicting mathematical definitions of \"fairness\", we should instead ask - what does fairness mean in a given context and how does the outcome affect individuals and the broader society.\n\n## References\n\n[^ref]\n",
    "code": "var Component=(()=>{var cn=Object.create;var S=Object.defineProperty;var bn=Object.getOwnPropertyDescriptor;var un=Object.getOwnPropertyNames;var mn=Object.getPrototypeOf,fn=Object.prototype.hasOwnProperty;var H=(l,e)=>()=>(e||l((e={exports:{}}).exports,e),e.exports),hn=(l,e)=>{for(var m in e)S(l,m,{get:e[m],enumerable:!0})},Ne=(l,e,m,_)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let x of un(e))!fn.call(l,x)&&x!==m&&S(l,x,{get:()=>e[x],enumerable:!(_=bn(e,x))||_.enumerable});return l};var pn=(l,e,m)=>(m=l!=null?cn(mn(l)):{},Ne(e||!l||!l.__esModule?S(m,\"default\",{value:l,enumerable:!0}):m,l)),_n=l=>Ne(S({},\"__esModule\",{value:!0}),l);var ve=H((vn,ke)=>{ke.exports=React});var we=H(z=>{\"use strict\";(function(){\"use strict\";var l=ve(),e=Symbol.for(\"react.element\"),m=Symbol.for(\"react.portal\"),_=Symbol.for(\"react.fragment\"),x=Symbol.for(\"react.strict_mode\"),K=Symbol.for(\"react.profiler\"),G=Symbol.for(\"react.provider\"),J=Symbol.for(\"react.context\"),I=Symbol.for(\"react.forward_ref\"),P=Symbol.for(\"react.suspense\"),O=Symbol.for(\"react.suspense_list\"),E=Symbol.for(\"react.memo\"),q=Symbol.for(\"react.lazy\"),Te=Symbol.for(\"react.offscreen\"),X=Symbol.iterator,Re=\"@@iterator\";function Ae(a){if(a===null||typeof a!=\"object\")return null;var t=X&&a[X]||a[Re];return typeof t==\"function\"?t:null}var k=l.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function f(a){{for(var t=arguments.length,i=new Array(t>1?t-1:0),d=1;d<t;d++)i[d-1]=arguments[d];je(\"error\",a,i)}}function je(a,t,i){{var d=k.ReactDebugCurrentFrame,s=d.getStackAddendum();s!==\"\"&&(t+=\"%s\",i=i.concat([s]));var c=i.map(function(o){return String(o)});c.unshift(\"Warning: \"+t),Function.prototype.apply.call(console[a],console,c)}}var Ce=!1,Se=!1,Pe=!1,Oe=!1,qe=!1,Z;Z=Symbol.for(\"react.module.reference\");function Fe(a){return!!(typeof a==\"string\"||typeof a==\"function\"||a===_||a===K||qe||a===x||a===P||a===O||Oe||a===Te||Ce||Se||Pe||typeof a==\"object\"&&a!==null&&(a.$$typeof===q||a.$$typeof===E||a.$$typeof===G||a.$$typeof===J||a.$$typeof===I||a.$$typeof===Z||a.getModuleId!==void 0))}function Me(a,t,i){var d=a.displayName;if(d)return d;var s=t.displayName||t.name||\"\";return s!==\"\"?i+\"(\"+s+\")\":i}function Q(a){return a.displayName||\"Context\"}function y(a){if(a==null)return null;if(typeof a.tag==\"number\"&&f(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),typeof a==\"function\")return a.displayName||a.name||null;if(typeof a==\"string\")return a;switch(a){case _:return\"Fragment\";case m:return\"Portal\";case K:return\"Profiler\";case x:return\"StrictMode\";case P:return\"Suspense\";case O:return\"SuspenseList\"}if(typeof a==\"object\")switch(a.$$typeof){case J:var t=a;return Q(t)+\".Consumer\";case G:var i=a;return Q(i._context)+\".Provider\";case I:return Me(a,a.render,\"ForwardRef\");case E:var d=a.displayName||null;return d!==null?d:y(a.type)||\"Memo\";case q:{var s=a,c=s._payload,o=s._init;try{return y(o(c))}catch{return null}}}return null}var N=Object.assign,D=0,ee,ne,ae,te,ie,de,re;function oe(){}oe.__reactDisabledLog=!0;function We(){{if(D===0){ee=console.log,ne=console.info,ae=console.warn,te=console.error,ie=console.group,de=console.groupCollapsed,re=console.groupEnd;var a={configurable:!0,enumerable:!0,value:oe,writable:!0};Object.defineProperties(console,{info:a,log:a,warn:a,error:a,group:a,groupCollapsed:a,groupEnd:a})}D++}}function Be(){{if(D--,D===0){var a={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:N({},a,{value:ee}),info:N({},a,{value:ne}),warn:N({},a,{value:ae}),error:N({},a,{value:te}),group:N({},a,{value:ie}),groupCollapsed:N({},a,{value:de}),groupEnd:N({},a,{value:re})})}D<0&&f(\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\")}}var F=k.ReactCurrentDispatcher,M;function T(a,t,i){{if(M===void 0)try{throw Error()}catch(s){var d=s.stack.trim().match(/\\n( *(at )?)/);M=d&&d[1]||\"\"}return`\n`+M+a}}var W=!1,R;{var Ve=typeof WeakMap==\"function\"?WeakMap:Map;R=new Ve}function se(a,t){if(!a||W)return\"\";{var i=R.get(a);if(i!==void 0)return i}var d;W=!0;var s=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var c;c=F.current,F.current=null,We();try{if(t){var o=function(){throw Error()};if(Object.defineProperty(o.prototype,\"props\",{set:function(){throw Error()}}),typeof Reflect==\"object\"&&Reflect.construct){try{Reflect.construct(o,[])}catch(g){d=g}Reflect.construct(a,[],o)}else{try{o.call()}catch(g){d=g}a.call(o.prototype)}}else{try{throw Error()}catch(g){d=g}a()}}catch(g){if(g&&d&&typeof g.stack==\"string\"){for(var r=g.stack.split(`\n`),h=d.stack.split(`\n`),b=r.length-1,u=h.length-1;b>=1&&u>=0&&r[b]!==h[u];)u--;for(;b>=1&&u>=0;b--,u--)if(r[b]!==h[u]){if(b!==1||u!==1)do if(b--,u--,u<0||r[b]!==h[u]){var p=`\n`+r[b].replace(\" at new \",\" at \");return a.displayName&&p.includes(\"<anonymous>\")&&(p=p.replace(\"<anonymous>\",a.displayName)),typeof a==\"function\"&&R.set(a,p),p}while(b>=1&&u>=0);break}}}finally{W=!1,F.current=c,Be(),Error.prepareStackTrace=s}var w=a?a.displayName||a.name:\"\",xe=w?T(w):\"\";return typeof a==\"function\"&&R.set(a,xe),xe}function Ye(a,t,i){return se(a,!1)}function $e(a){var t=a.prototype;return!!(t&&t.isReactComponent)}function A(a,t,i){if(a==null)return\"\";if(typeof a==\"function\")return se(a,$e(a));if(typeof a==\"string\")return T(a);switch(a){case P:return T(\"Suspense\");case O:return T(\"SuspenseList\")}if(typeof a==\"object\")switch(a.$$typeof){case I:return Ye(a.render);case E:return A(a.type,t,i);case q:{var d=a,s=d._payload,c=d._init;try{return A(c(s),t,i)}catch{}}}return\"\"}var j=Object.prototype.hasOwnProperty,le={},ce=k.ReactDebugCurrentFrame;function C(a){if(a){var t=a._owner,i=A(a.type,a._source,t?t.type:null);ce.setExtraStackFrame(i)}else ce.setExtraStackFrame(null)}function Le(a,t,i,d,s){{var c=Function.call.bind(j);for(var o in a)if(c(a,o)){var r=void 0;try{if(typeof a[o]!=\"function\"){var h=Error((d||\"React class\")+\": \"+i+\" type `\"+o+\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\"+typeof a[o]+\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\");throw h.name=\"Invariant Violation\",h}r=a[o](t,o,d,i,null,\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\")}catch(b){r=b}r&&!(r instanceof Error)&&(C(s),f(\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\",d||\"React class\",i,o,typeof r),C(null)),r instanceof Error&&!(r.message in le)&&(le[r.message]=!0,C(s),f(\"Failed %s type: %s\",i,r.message),C(null))}}}var He=Array.isArray;function B(a){return He(a)}function ze(a){{var t=typeof Symbol==\"function\"&&Symbol.toStringTag,i=t&&a[Symbol.toStringTag]||a.constructor.name||\"Object\";return i}}function Ke(a){try{return be(a),!1}catch{return!0}}function be(a){return\"\"+a}function ue(a){if(Ke(a))return f(\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\",ze(a)),be(a)}var U=k.ReactCurrentOwner,Ge={key:!0,ref:!0,__self:!0,__source:!0},me,fe,V;V={};function Je(a){if(j.call(a,\"ref\")){var t=Object.getOwnPropertyDescriptor(a,\"ref\").get;if(t&&t.isReactWarning)return!1}return a.ref!==void 0}function Xe(a){if(j.call(a,\"key\")){var t=Object.getOwnPropertyDescriptor(a,\"key\").get;if(t&&t.isReactWarning)return!1}return a.key!==void 0}function Ze(a,t){if(typeof a.ref==\"string\"&&U.current&&t&&U.current.stateNode!==t){var i=y(U.current.type);V[i]||(f('Component \"%s\" contains the string ref \"%s\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',y(U.current.type),a.ref),V[i]=!0)}}function Qe(a,t){{var i=function(){me||(me=!0,f(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",t))};i.isReactWarning=!0,Object.defineProperty(a,\"key\",{get:i,configurable:!0})}}function en(a,t){{var i=function(){fe||(fe=!0,f(\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",t))};i.isReactWarning=!0,Object.defineProperty(a,\"ref\",{get:i,configurable:!0})}}var nn=function(a,t,i,d,s,c,o){var r={$$typeof:e,type:a,key:t,ref:i,props:o,_owner:c};return r._store={},Object.defineProperty(r._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(r,\"_self\",{configurable:!1,enumerable:!1,writable:!1,value:d}),Object.defineProperty(r,\"_source\",{configurable:!1,enumerable:!1,writable:!1,value:s}),Object.freeze&&(Object.freeze(r.props),Object.freeze(r)),r};function an(a,t,i,d,s){{var c,o={},r=null,h=null;i!==void 0&&(ue(i),r=\"\"+i),Xe(t)&&(ue(t.key),r=\"\"+t.key),Je(t)&&(h=t.ref,Ze(t,s));for(c in t)j.call(t,c)&&!Ge.hasOwnProperty(c)&&(o[c]=t[c]);if(a&&a.defaultProps){var b=a.defaultProps;for(c in b)o[c]===void 0&&(o[c]=b[c])}if(r||h){var u=typeof a==\"function\"?a.displayName||a.name||\"Unknown\":a;r&&Qe(o,u),h&&en(o,u)}return nn(a,r,h,s,d,U.current,o)}}var Y=k.ReactCurrentOwner,he=k.ReactDebugCurrentFrame;function v(a){if(a){var t=a._owner,i=A(a.type,a._source,t?t.type:null);he.setExtraStackFrame(i)}else he.setExtraStackFrame(null)}var $;$=!1;function L(a){return typeof a==\"object\"&&a!==null&&a.$$typeof===e}function pe(){{if(Y.current){var a=y(Y.current.type);if(a)return`\n\nCheck the render method of \\``+a+\"`.\"}return\"\"}}function tn(a){{if(a!==void 0){var t=a.fileName.replace(/^.*[\\\\\\/]/,\"\"),i=a.lineNumber;return`\n\nCheck your code at `+t+\":\"+i+\".\"}return\"\"}}var _e={};function dn(a){{var t=pe();if(!t){var i=typeof a==\"string\"?a:a.displayName||a.name;i&&(t=`\n\nCheck the top-level render call using <`+i+\">.\")}return t}}function ye(a,t){{if(!a._store||a._store.validated||a.key!=null)return;a._store.validated=!0;var i=dn(t);if(_e[i])return;_e[i]=!0;var d=\"\";a&&a._owner&&a._owner!==Y.current&&(d=\" It was passed a child from \"+y(a._owner.type)+\".\"),v(a),f('Each child in a list should have a unique \"key\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',i,d),v(null)}}function ge(a,t){{if(typeof a!=\"object\")return;if(B(a))for(var i=0;i<a.length;i++){var d=a[i];L(d)&&ye(d,t)}else if(L(a))a._store&&(a._store.validated=!0);else if(a){var s=Ae(a);if(typeof s==\"function\"&&s!==a.entries)for(var c=s.call(a),o;!(o=c.next()).done;)L(o.value)&&ye(o.value,t)}}}function rn(a){{var t=a.type;if(t==null||typeof t==\"string\")return;var i;if(typeof t==\"function\")i=t.propTypes;else if(typeof t==\"object\"&&(t.$$typeof===I||t.$$typeof===E))i=t.propTypes;else return;if(i){var d=y(t);Le(i,a.props,\"prop\",d,a)}else if(t.PropTypes!==void 0&&!$){$=!0;var s=y(t);f(\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\",s||\"Unknown\")}typeof t.getDefaultProps==\"function\"&&!t.getDefaultProps.isReactClassApproved&&f(\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\")}}function on(a){{for(var t=Object.keys(a.props),i=0;i<t.length;i++){var d=t[i];if(d!==\"children\"&&d!==\"key\"){v(a),f(\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\",d),v(null);break}}a.ref!==null&&(v(a),f(\"Invalid attribute `ref` supplied to `React.Fragment`.\"),v(null))}}function sn(a,t,i,d,s,c){{var o=Fe(a);if(!o){var r=\"\";(a===void 0||typeof a==\"object\"&&a!==null&&Object.keys(a).length===0)&&(r+=\" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.\");var h=tn(s);h?r+=h:r+=pe();var b;a===null?b=\"null\":B(a)?b=\"array\":a!==void 0&&a.$$typeof===e?(b=\"<\"+(y(a.type)||\"Unknown\")+\" />\",r=\" Did you accidentally export a JSX literal instead of a component?\"):b=typeof a,f(\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\",b,r)}var u=an(a,t,i,s,c);if(u==null)return u;if(o){var p=t.children;if(p!==void 0)if(d)if(B(p)){for(var w=0;w<p.length;w++)ge(p[w],a);Object.freeze&&Object.freeze(p)}else f(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else ge(p,a)}return a===_?on(u):rn(u),u}}var ln=sn;z.Fragment=_,z.jsxDEV=ln})()});var Ue=H((Dn,De)=>{\"use strict\";De.exports=we()});var Nn={};hn(Nn,{default:()=>xn,frontmatter:()=>yn});var n=pn(Ue()),yn={title:\"A Human-centric Approach to Fairness in AI\",date:\"2021-09-16\",lastmod:\"2021-12-24\",tags:[\"fairness\",\"ethics\",\"veritas\",\"philosophy\",\"responsible-ai\",\"data science\",\"singapore\"],draft:!1,bibliography:\"responsible-ai.bib\",summary:\"Fairness is messy and complicated. Attempts to distil it down to a single metric is unhelpful and counter-productive. As business owners and model developers we should embrace the struggle in trying to apply fairness in artificial intelligence and data analytics models.\",images:[\"/static/img/fairness/human-centric-approach-to-ai-cover.png\"],layout:\"PostLayout\"};function Ie(l){let e=Object.assign({h2:\"h2\",a:\"a\",span:\"span\",p:\"p\",sup:\"sup\",ol:\"ol\",li:\"li\",strong:\"strong\",em:\"em\",div:\"div\",blockquote:\"blockquote\",h3:\"h3\",i:\"i\",section:\"section\"},l.components),{TOCInline:m,Image:_}=e;return _||Ee(\"Image\",!0,\"47:1-47:70\"),m||Ee(\"TOCInline\",!0,\"13:1-13:57\"),(0,n.jsxDEV)(n.Fragment,{children:[(0,n.jsxDEV)(m,{toc:l.toc,asDisclosure:!0,toHeading:3},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:13,columnNumber:1},this),(0,n.jsxDEV)(e.h2,{id:\"background\",children:[(0,n.jsxDEV)(e.a,{href:\"#background\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Background\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:15,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"After 4+ years implementing data science solutions, I find myself confronting a familiar set of challenges in which I started my career in - policy evaluation. While this is in the sphere of AI fairness, there are many similar parallels across both domains.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:17,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"This post is written as part of our participation in the Monetary Authority of Singapore (MAS) Veritas challenge on applying fairness, ethics, accountability and fairness (FEAT) in artificial intelligence and data analytics (AIDA) use cases.\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-1\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-1\",children:\"1\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:19,columnNumber:242},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:19,columnNumber:242},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:19,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"As one of the shortlisted finalist, I thought it would be good to blog about the approach we are taking and why we believe it to be the right one. In the initial written proposal, rather than simply pitching our existing experience and expertise, I decided to write an aspirational proposal of what fairness in machine learning could look like, which I feel is very different from what it currently is in practice. I thank the MAS and organising committee for giving us the opportunity to shape some of the most important policy guidelines and recommendations in the space.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:21,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"Our proposal is novel in a few ways:\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:23,columnNumber:1},this),(0,n.jsxDEV)(e.ol,{children:[(0,n.jsxDEV)(e.li,{children:(0,n.jsxDEV)(e.p,{children:[\"It is an \",(0,n.jsxDEV)(e.strong,{children:\"open-source solution\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:25,columnNumber:13},this),\". We are transparent by design. It strikes me as odd that methods to evaluate whether a model is fair or not is shrouded in secrecy by commercial companies.\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-2\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-2\",children:\"2\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:25,columnNumber:193},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:25,columnNumber:193},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:25,columnNumber:4},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:25,columnNumber:1},this),(0,n.jsxDEV)(e.li,{children:(0,n.jsxDEV)(e.p,{children:[\"We do \",(0,n.jsxDEV)(e.strong,{children:\"not\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:27,columnNumber:10},this),\" make ethical judgements of what is fair. We believe that in the issue of fairness there is no one metric to rule them all. Rather a deliberative and consultative process is essential. Though having made a decision, we can make subsequent workstream processes as easy as possible.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:27,columnNumber:4},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:27,columnNumber:1},this),(0,n.jsxDEV)(e.li,{children:(0,n.jsxDEV)(e.p,{children:[\"We embed the evaluation of AI fairness within the \",(0,n.jsxDEV)(e.strong,{children:\"best practices\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:29,columnNumber:54},this),\" of machine learning development and operations such as version control, unit testing and continuous integration.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:29,columnNumber:4},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:29,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:25,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"It has been a fun few weeks exploring the latest thinking of scholars in this field and listening to the masterclass sessions provided by the industry partners. In the spirit of openness, I decided to document my thought process in designing the described framework. This is the first post in a series of three.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:31,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"For the first post, I will focus on the issue of fairness and argue that the right approach that should be adopted by the industry is a human-centric one. Attempts to distil fairness down to a single metric are unhelpful and counter-productive. As business owners and model developers we should embrace the struggle in trying to define fairness in relation to the problem we are trying to solve, the models that we are using, and the broader societal context, rather than shun it.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:33,columnNumber:1},this),(0,n.jsxDEV)(e.h2,{id:\"the-case-for-a-single-fairness-metric\",children:[(0,n.jsxDEV)(e.a,{href:\"#the-case-for-a-single-fairness-metric\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"The case for a single fairness metric\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:35,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"Having a single metric to quantify what is fair or not seems to be a goal for many AI scholars in the field, as well as many of industry practitioners. After all, while methods might vary, the foundation of machine learning hinges on maximising or minimising clear objectives\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-3\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-3\",children:\"3\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:37,columnNumber:276},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:37,columnNumber:276},this),\". A business problem can then be translated to a mathematical / statistical / computational problem and different methods can be compared against each other based on how far the predicted output differs from the actual output as measured by the objective function.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:37,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"If fairness could be distilled down to a single metric, a data scientist can include it as part of the objective function or as a constraint and find an ideal point that maximises overall business needs while satisfying fairness requirements.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:39,columnNumber:1},this),(0,n.jsxDEV)(e.h2,{id:\"a-brief-survey-of-fairness-metrics\",children:[(0,n.jsxDEV)(e.a,{href:\"#a-brief-survey-of-fairness-metrics\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"A brief survey of fairness metrics\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:41,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.span,{id:\"citation--barocas-hardt-narayanan--1\",children:(0,n.jsxDEV)(e.a,{href:\"#bib-barocas-hardt-narayanan\",children:\"Barocas et al. (2019)\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\" lists 19 demographic fairness criteria for classification problems. This includes measures such as \",(0,n.jsxDEV)(e.em,{children:\"Demographic Parity / Statistical Parity\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:43,columnNumber:125},this),\" \",(0,n.jsxDEV)(e.span,{id:\"citation--dwork2012--2\",children:[\"(\",(0,n.jsxDEV)(e.a,{href:\"#bib-dwork2012\",children:\"Dwork et al., 2012\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\")\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\", \",(0,n.jsxDEV)(e.em,{children:\"Equalized Odds Metric\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:43,columnNumber:181},this),\" \",(0,n.jsxDEV)(e.span,{id:\"citation--hardt-price-srebo--3\",children:[\"(\",(0,n.jsxDEV)(e.a,{href:\"#bib-hardt-price-srebo\",children:\"Hardt et al., 2016\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\")\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\" and \",(0,n.jsxDEV)(e.em,{children:\"Calibration within Groups\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:43,columnNumber:230},this),\" \",(0,n.jsxDEV)(e.span,{id:\"citation--chouldechova2017--4\",children:[\"(\",(0,n.jsxDEV)(e.a,{href:\"#bib-chouldechova2017\",children:\"Chouldechova, 2017\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\")\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\". They are all statistical measures derived from the predictions of a classification model and differ in terms of which element(s) of the confusion matrix they are trying to test for equivalence.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:43,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"In another survey of fairness definitions, \",(0,n.jsxDEV)(e.span,{id:\"citation--verma-rubin--5\",children:(0,n.jsxDEV)(e.a,{href:\"#bib-verma-rubin\",children:\"Verma & Rubin (2018)\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\" listed 20 definitions of fairness, 13 belonging to statistical measures, 3 being classified as similarity-based measures and the remaining 4 stemming from causal reasoning. Applying a logistic regression to a German credit dataset, they showed that approximately half of their definitions are satisfied while the other half are not satisfied:\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:45,columnNumber:1},this),(0,n.jsxDEV)(e.div,{children:(0,n.jsxDEV)(_,{alt:\"fairness-metrics-verma-rubin\",src:\"/static/img/fairness/verma-table.png\",width:\"492\",height:\"507\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:47,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:47,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"Concluding their assessment of their analysis, they wrote:\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:49,columnNumber:1},this),(0,n.jsxDEV)(e.blockquote,{children:(0,n.jsxDEV)(e.p,{children:\"So, is the classifier fair? Clearly, the answer to this question depends on the notion of fairness one wants to adopt. We believe more work is needed to clarify which definitions are appropriate to each particular situation.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:51,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:51,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.span,{id:\"citation--saxena2019--6\",children:(0,n.jsxDEV)(e.a,{href:\"#bib-saxena2019\",children:\"Saxena et al. (2019)\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\", and \",(0,n.jsxDEV)(e.span,{id:\"citation--srivastava2019--7\",children:(0,n.jsxDEV)(e.a,{href:\"#bib-srivastava2019\",children:\"Srivastava et al. (2019)\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\" conducted experiments to find out how public attitudes map to fairness definitions for machine learning. The first paper found calibrated fairness tends to be preferred in the context of loan decisions while the second study found that demographic parity most closely matches people's idea of fairness as applied to criminal risk prediction and skin cancer risk prediction.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:53,columnNumber:1},this),(0,n.jsxDEV)(e.div,{children:(0,n.jsxDEV)(_,{alt:\"srivastava-fairness-ui\",src:\"/static/img/fairness/srivastava-fairness-question.png\",width:\"1160\",height:\"368\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:55,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:55,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"While the intentions of the research were right and is an interesting area to explore, the methodology is lacking. Both papers ask respondents sourced through Amazon's mechanical turk to answer a series of questions. As the questions very explicitly mentioned race and gender attributes, respondents are primed to perceive fairness through the lens of a redistributive one. The most one could claim is that if subjects are forced to reason about fairness in a utilitarian cost-benefit setting, they tend to prefer the selected metrics.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:57,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"The right approach I argue would be to take a more holistic view of what's fair and ethical, before re-introducing the issue of what metrics. In the subsequent sections, I highlight the limitations with the existing approach by pointing out three facts and explore their implications:\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:59,columnNumber:1},this),(0,n.jsxDEV)(e.ol,{children:[(0,n.jsxDEV)(e.li,{children:\"Data as a reflection of ourselves and society\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:61,columnNumber:1},this),(0,n.jsxDEV)(e.li,{children:\"Models as tools for discrimination\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:62,columnNumber:1},this),(0,n.jsxDEV)(e.li,{children:\"The difficulty of agreeing on a common definition of what's ethical\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:63,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:61,columnNumber:1},this),(0,n.jsxDEV)(e.h2,{id:\"data-as-mirrors\",children:[(0,n.jsxDEV)(e.a,{href:\"#data-as-mirrors\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Data as mirrors\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:65,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"In a discussion on automated hiring systems and biases\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-4\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-4\",children:\"4\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:67,columnNumber:55},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:67,columnNumber:55},this),\" in the hiring process, \",(0,n.jsxDEV)(e.span,{id:\"citation--stoyanovich-howe-jagadish--8\",children:(0,n.jsxDEV)(e.a,{href:\"#bib-stoyanovich-howe-jagadish\",children:\"Stoyanovich et al. (2020)\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\" eloquently warns about the inherent biases of a dataset using an analogy of data as a mirrored reflection of the world:\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:67,columnNumber:1},this),(0,n.jsxDEV)(e.blockquote,{children:(0,n.jsxDEV)(e.p,{children:\"Informally, data is a mirror reflection of the world. More often than not, this reflection is distorted. One reason for this may be that the mirror itself (the measurement process) is distorted: it faithfully reflects some portions of the world, while amplifying or diminishing others. Another reason may be that even if the mirror was perfect, it may be reflecting a distorted world \\u2014 a world such as it is, and not as it could or should be.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:69,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:69,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"A data-centric approach is inherently limited, as it lacks information about the context in which it is generated, processed and analysed. Just like how a reflection does not know it is distorted, or why it is distorted, we should not expect the data to know information about itself. While some might think collecting more data might alleviate data issues, the size of the data can be regarded as the size of the mirror - a larger mirror does not mean that the image is less distorted.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:71,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"Similarly, we cannot expect an algorithm to make a judgement of what is fair or not. While, I shall argue in the subsequent sections that even if the algorithm has complete information about the context in which the data is generated it should still not be able to make such a judgement, the fact that it lacks such historical context and understanding should already be red flags for anyone trying to think that it could.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:73,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"A dataset forces a particular perspective of reality. It does not tell us what is not captured or other possible alternatives. This is not to say that the reflection observed is not important - it is, but we need to bring in additional perspectives and dimensions to contextualise our understanding.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:75,columnNumber:1},this),(0,n.jsxDEV)(e.h2,{id:\"models-as-discriminatory\",children:[(0,n.jsxDEV)(e.a,{href:\"#models-as-discriminatory\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Models as discriminatory\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:77,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"Every model is discriminatory.\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-5\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-5\",children:\"5\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:79,columnNumber:31},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:79,columnNumber:31},this),\" In the process of trying to optimise against a particular objective function, it tries its best to discriminate against all possible dimensions available in a dataset. This applies to complicated deep neural networks, simpler regression trees and even heuristic models.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:79,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"In the context of a hiring decision, the model might make a classification based on input features such as test scores, psychometric scores, interviewer ratings and of course age, gender and race. The ability of more complicated models to discriminate is the main rationale for the use of more powerful and deeper machine learning models.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:81,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"Sometimes it discriminates too much and is not effective in real world applications - i.e. the model has overfitted the dataset. Hence, the use of a hold-out test set to cross-validate the results and ensure it is able to generalize more broadly in world world cases. Nonetheless, left unconstrained, a model will pursue the single goal it is created for, discriminate against all attributes to maximise an objective function, including attributes such as race, gender or religion.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:83,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"Even if such attributes are omitted, the model would also be able to learn existing attributes in a dataset and its correlation with the protected attributes. \",(0,n.jsxDEV)(e.span,{id:\"citation--barocas-hardt-narayanan--9\",children:(0,n.jsxDEV)(e.a,{href:\"#bib-barocas-hardt-narayanan\",children:\"Barocas et al. (2019)\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\" gives an example of how browsing data might be used differentiate the sexes. While each website provides a small signal, their joint distribution can strongly predict whether a user is male or female.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:85,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"If models are inherently discriminatory, it is up to humans to decide what should be discriminated against and what is not.\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-6\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-6\",children:\"6\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:87,columnNumber:124},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:87,columnNumber:124},this),\" Relying on a single utility function to determine the tradeoffs between fairness and efficiency seems to be a way of evading responsibility and accountability.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:87,columnNumber:1},this),(0,n.jsxDEV)(e.h2,{id:\"ethics-and-fairness\",children:[(0,n.jsxDEV)(e.a,{href:\"#ethics-and-fairness\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Ethics and Fairness\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:89,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"The debate on ethics, fairness and justice is not new and has been contested by philosophers and political scientists from Plato to Rawls. I will probably not do justice in summarising the nuances in the debate, but I think it's still worth highlighting a few different perspectives to show how standards of what is fair varies.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:91,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:['Before we can dive into the question of what is fair or equitable, we have to ask \"fair\" by what values or standards?',(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-7\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-7\",children:\"7\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:93,columnNumber:118},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:93,columnNumber:118},this),\" The issue of what is right or wrong brings us into the heart of ethics.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:93,columnNumber:1},this),(0,n.jsxDEV)(e.h3,{id:\"utilitarianism\",children:[(0,n.jsxDEV)(e.a,{href:\"#utilitarianism\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Utilitarianism\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:95,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"One way that we could determine whether a particular act is right would be by summing up the benefits and substracting the cost of its associated harms. If the sum of such a measure (utility) across all affected individuals is positive, it is ethically correct.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:97,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"Intuitively, this definition of \",(0,n.jsxDEV)(e.strong,{children:\"act utilitarianism\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:99,columnNumber:33},this),\" maps most closely to what is possible by the single fairness metric formulation. If we could list out the benefits and harms of every outcome of a models decision, we could create a single utility function that maximises that objective, and in the process of doing so, we maximise social utility.\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-8\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-8\",children:\"8\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:99,columnNumber:352},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:99,columnNumber:352},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:99,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"Alternatively, we could take a step back and ask what would happen if every person / company were to adopt such a rule or policy, would it be in the best interest of society? This line of reasoning, also known as \",(0,n.jsxDEV)(e.strong,{children:\"rule utilitarianism\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:101,columnNumber:214},this),\" seeks to ground the mathematical formulation of act utilitarianism with some sense of check and balances.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:101,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"While possibly easier to implement as it relates it AI systems, utilitarianism has major gaps, both practically and morally. Practically, it is nigh impossible to measure benefits and harms across disparate areas (e.g. health vs wealth), or agree on a suitable discount factor to consider the longer lasting effects of policies. Morally, it is hard to be convinced that we should only be concerning ourselves with the consequences of an action and not the action itself. It is also easy to imagine a scenario where a model can be deployed where the benefits to the majority outweigh the harm to a minority class. Morally such as an action would probably strike us as unfair but it would be ethical in a utilitarian framework.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:103,columnNumber:1},this),(0,n.jsxDEV)(e.h3,{id:\"deontological-ethics\",children:[(0,n.jsxDEV)(e.a,{href:\"#deontological-ethics\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Deontological Ethics\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:105,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"In a utilitarian framework, every consequence can be measured and compared, including the value of a human life. This might strike us as unsettling, not because it is hard to put a price tag to life, but just because we think that human life should not be valued and compared. For an AI self-driving car, when faced with a decision of serving to save a life, it should not be calculating the resulting damage and weighing it against the value of a human life. Instead, it should serve, just because saving a life is the right thing to do.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:107,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"Such a way of thinking falls squarely in the realm of duty ethics or deontology. Some choices cannot be justified by their effects, no matter how good their consequences are - right takes priority over good.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:109,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"While there are variations within the deontological ethics community such, whether to focus on an agent's duties or a victim's rights, they share in common a belief as espoused by \",(0,n.jsxDEV)(e.span,{id:\"citation--kant1785--10\",children:(0,n.jsxDEV)(e.a,{href:\"#bib-kant1785\",children:\"Kant (1785)\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\" that the only thing unqualifiedly good is a good will.\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-9\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-9\",children:\"9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:111,columnNumber:245},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:111,columnNumber:245},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:111,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"For the rest of human decision making that does not classify as moral decisions, there is no categorical imperative to do anything associated about it.\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-10\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-10\",children:\"10\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:113,columnNumber:152},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:113,columnNumber:152},this),\" This seems like a more sane basis to ground our decision making process on and seems to be more aligned with moral intuitions of society in general. Practically, this translates to a different way of evaluating models - if the system does not make decisions that contradict what we believe as categorical imperatives then there is no reason not to give the machine the flexibility to optimize.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:113,columnNumber:1},this),(0,n.jsxDEV)(e.h3,{id:\"virtues\",children:[(0,n.jsxDEV)(e.a,{href:\"#virtues\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Virtues\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:115,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"Instead of thinking about what goodness and ethics might mean for a machine, we can ask what are the characteristics of a good person. Through that lens, we might say that a person is ethical if he lives by certain traits or virtues which one might regard as morally good.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:117,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"The apparent simplicity and practicality of such a system of morality are also its greatest strengths. \",(0,n.jsxDEV)(e.span,{id:\"citation--foot1997--11\",children:(0,n.jsxDEV)(e.a,{href:\"#bib-foot1997\",children:\"Foot (1997)\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\" in Virtue and vices, lists three essential feature of a virtue:\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:119,columnNumber:1},this),(0,n.jsxDEV)(e.ol,{children:[(0,n.jsxDEV)(e.li,{children:\"It's a disposition of the will\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:121,columnNumber:1},this),(0,n.jsxDEV)(e.li,{children:\"It's beneficial to others, or to its possessor as well as to others\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:122,columnNumber:1},this),(0,n.jsxDEV)(e.li,{children:\"It is corrective of some bad general human tendency\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:123,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:121,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"This list might correspond to virtues like Aristotle's Nicomachean Ethics which includes values such as Courage, Magnanimity and Truthfulness, or it could correspond to theological values of the church - Faith, Hope and Charity.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:125,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"In this age of technology and automation, \",(0,n.jsxDEV)(e.span,{id:\"citation--vallor2016--12\",children:(0,n.jsxDEV)(e.a,{href:\"#bib-vallor2016\",children:\"Vallor (2016)\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),' argues that virtue ethics can be a shining beacon that guides decision makers in techno-sociological choices as it is \"ideally suited for adaptation to the open-ended and varied encounters with particular technologies that will shape the human condition in this and coming centuries\". While agreeing on a common standard of shared virtue ethic might be a herculean task, it is not unreasonable to expect companies and individuals to have a set of core values that they believe in, and would form the basis of evaluating the impact of AIDA models.',(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-11\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-11\",children:\"11\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:127,columnNumber:601},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:127,columnNumber:601},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:127,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"One might ask, how would it be possible to embed moral virtues in an artificial intelligence system and question whether that can be operationalised. Rather than expecting a system to learn a set of values, I think it makes more sense to ask whether the outcomes produced by these systems conform to our internal values. Instead of asking whether a gun is ethical, we should ask in what context and what circumstances is the use of a gun ethical.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:129,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"Operationally, instead of calculating the expected utility of benefits and harms, these benefits and harms could be presented to an internal review board and accessed based on a set of virtues. If the organisation has virtues such as justice, empathy or care on their checklist, they would be able to flag out a scenario where the majority benefits at the expense of a minority group. If such values are not present, we can then turn the discussion towards the values of an organisation, rather than to talk about AI values which is a pointless exercise.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:131,columnNumber:1},this),(0,n.jsxDEV)(e.h3,{id:\"practical-ethics\",children:[(0,n.jsxDEV)(e.a,{href:\"#practical-ethics\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Practical Ethics\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:133,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"The discussion above on three popular ethical schools of thought is not meant to promote one over the other. Practically, human beings have a deeply complex value and judgement system which might not even be internally consistent. In certain cases, one might adopt a utilitarian point of view, in other situations, we might choose to adhere to certain religious principles and beliefs.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:135,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"One thing it does show is that what is ethical or not can be highly contentious and we should not restrict ourselves to a single metric in trying to categorise what is right or wrong. Not only is it incapable of capturing the crux of ethical arguments put forth in such debates, but it also artificially restricts us to a set of measures that might not map to what society perceives as ethical.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:137,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"Next, we turn our attention to the issue of fairness since it is often brought up in discussions on AI governance. We have already seen it mentioned in the above section on virtue ethics and one might claim that we ought to be fair in the application of AI systems, but what do we exactly mean by fairness?\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-12\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-12\",children:\"12\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:139,columnNumber:307},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:139,columnNumber:307},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:139,columnNumber:1},this),(0,n.jsxDEV)(e.h3,{id:\"fairness\",children:[(0,n.jsxDEV)(e.a,{href:\"#fairness\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Fairness\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:141,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"Aristotle's \",(0,n.jsxDEV)(e.em,{children:\"principle of equality\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:143,columnNumber:13},this),\" states that:\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:143,columnNumber:1},this),(0,n.jsxDEV)(e.blockquote,{children:(0,n.jsxDEV)(e.p,{children:\"equals should be treated equally and unequals unequally.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:145,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:145,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"The quote suggests that individuals should be treated the same, unless they differ in ways that are relevant to the situation in which they are involved. For example, if both Jack and Jill can do the work to the same standard, they should have the same chance of being hired. If Jack has a higher chance of being hired just by being a man, we can say that it is a case of unfair hiring.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:147,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"However, the principle of equality does not take into account individual differences that play a part in explaining one's circumstances. In the context of hiring, the opportunities available to women might vary significantly from those available to a man. These differences accumulate over time and translate to differences in work output, even if both men and women were initially equally capable and talented at birth. One might argue that fairness, in this case, is equity - everyone should have equal access to the same opportunities.\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-13\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-13\",children:\"13\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:149,columnNumber:539},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:149,columnNumber:539},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:149,columnNumber:1},this),(0,n.jsxDEV)(e.h2,{id:\"fairness-in-ai-systems\",children:[(0,n.jsxDEV)(e.a,{href:\"#fairness-in-ai-systems\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Fairness in AI systems\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:151,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"In the context of AI decision making systems, concepts of equality or equity deserve additional scrutiny. The metrics that are used to determine what is fair or not, are statistical measures that are derived based on the observed and collected data. Despite the seemingly numerous ways in which one can derive such metrics of fairness, these measures take the state of the world as given, rather than as constructed.\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-14\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-14\",children:\"14\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:153,columnNumber:417},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:153,columnNumber:417},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:153,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:['If that is the case, it would be better to name these measures as \"equivalence metrics\" rather than \"fairness metrics\" as they do not map to an ethical sense of fairness. We can ask whether the number of males and females recommended by our AI model is the same or if the estimated false positive or false negative rates are equivalent, but not whether the model is actually ',(0,n.jsxDEV)(e.em,{children:\"fair\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:155,columnNumber:376},this),\". Fairness requires situating the equivalence metric in the context that it is being applied, understanding historical or structural factors that could possible explain the differences and a value judgement based on some ethical reasoning.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:155,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:[\"Another benefit of using the term equivalence rather than fairness is that many of these measures are simply incompatible. Degenerate solutions notwithstanding, mathematically, as formulated in a classification problem, one cannot have equivalence across classes in the number of predicted positives as well as equivalence in the false positive or false negative rates.\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-15\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-15\",children:\"15\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:157,columnNumber:370},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:157,columnNumber:370},this),\" More broadly, how we think about fairness varies across scenarios, but given a specific problem, we can probably articulate why we think a particular distribution or outcome is fair.\",(0,n.jsxDEV)(e.sup,{children:(0,n.jsxDEV)(e.a,{href:\"#user-content-fn-16\",\"aria-describedby\":\"footnote-label\",\"data-footnote-ref\":!0,id:\"user-content-fnref-16\",children:\"16\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:157,columnNumber:631},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:157,columnNumber:631},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:157,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"A lot of what is discussed in the machine learning literature touches on fairness (or rather equivalence in certain outcomes) between groups, yet this narrowly constricts fairness to the notion of equality. Of course, we should think about fairness in the context of prejudiced groups, but we should also ask whether it is fair to an individual. Adding constraints in models might lead to worse outcomes for other individuals. If the decision making processs has serious consequences e.g. a fraud model, it is hard to think that we are truly fair by forcing models to obtain an equal number of fraud cases across certain demographic attributes.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:159,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"Practically, we should be flipping the approach. One should first ask what fairness means in a given scenario before mapping it to a numerical measure, if applicable. In the case of hiring decisions, where AI is used as a pre-selection step, we might think shortlisting an equal proportion of males and females is fair. While in the case of fraud prevention models, fairness probably means getting the best result for each subgroup by reducing the false positive rate for each group as low as possible and not artificially trying to make them equal!\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:161,columnNumber:1},this),(0,n.jsxDEV)(e.h2,{id:\"conclusion\",children:[(0,n.jsxDEV)(e.a,{href:\"#conclusion\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Conclusion\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:163,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:\"In this post, I question the rationale of fairness metrics as presented in the AI and machine learning literature. I argue that given the nature of data as (distorted) reflections of society, of models as unthinking agents of discrimination (in the broadest sense of the word), we should evaluate such systems from a human-centric perspective.\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:165,columnNumber:1},this),(0,n.jsxDEV)(e.p,{children:'While there is much debate within ethics on what constitutes right and wrong, by using it as a starting point of our decision-making process, we re-introduce human decision making to a process that should be governed by humans. Instead of absconding from our human responsibility on making informed decisions and leaving it to conflicting mathematical definitions of \"fairness\", we should instead ask - what does fairness mean in a given context and how does the outcome affect individuals and the broader society.'},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:167,columnNumber:1},this),(0,n.jsxDEV)(e.h2,{id:\"references\",children:[(0,n.jsxDEV)(e.a,{href:\"#references\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"References\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:169,columnNumber:1},this),(0,n.jsxDEV)(e.div,{className:\"csl-bib-body references\",id:\"refs\",children:[(0,n.jsxDEV)(e.div,{className:\"csl-entry\",id:\"bib-barocas-hardt-narayanan\",children:[\"Barocas, S., Hardt, M., & Narayanan, A. (2019). \",(0,n.jsxDEV)(e.i,{children:\"Fairness and Machine Learning\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\". fairmlbook.org.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.div,{className:\"csl-entry\",id:\"bib-chouldechova2017\",children:[\"Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. \",(0,n.jsxDEV)(e.i,{children:\"Big Data\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\", \",(0,n.jsxDEV)(e.i,{children:\"5\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"(2), 153\\u2013163.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.div,{className:\"csl-entry\",id:\"bib-dwork2012\",children:[\"Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012). Fairness through awareness. \",(0,n.jsxDEV)(e.i,{children:\"Proceedings of the 3rd Innovations in Theoretical Computer Science Conference\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\", 214\\u2013226.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.div,{className:\"csl-entry\",id:\"bib-foot1997\",children:[\"Foot, P. (1997). Virtues and vices. \",(0,n.jsxDEV)(e.i,{children:\"Virtue Ethics\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\", 163\\u2013177.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.div,{className:\"csl-entry\",id:\"bib-hardt-price-srebo\",children:[\"Hardt, M., Price, E., & Srebro, N. (2016). Equality of opportunity in supervised learning. \",(0,n.jsxDEV)(e.i,{children:\"Advances in Neural Information Processing Systems\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\", \",(0,n.jsxDEV)(e.i,{children:\"29\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\", 3315\\u20133323.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.div,{className:\"csl-entry\",id:\"bib-kant1785\",children:[\"Kant, I. (1785). \",(0,n.jsxDEV)(e.i,{children:\"The categorical imperative\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\".\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.div,{className:\"csl-entry\",id:\"bib-saxena2019\",children:[\"Saxena, N. A., Huang, K., DeFilippis, E., Radanovic, G., Parkes, D. C., & Liu, Y. (2019). How do fairness definitions fare? Examining public attitudes towards algorithmic definitions of fairness. \",(0,n.jsxDEV)(e.i,{children:\"Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\", 99\\u2013106.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.div,{className:\"csl-entry\",id:\"bib-srivastava2019\",children:[\"Srivastava, M., Heidari, H., & Krause, A. (2019). Mathematical notions vs. human perception of fairness: A descriptive approach to fairness for machine learning. \",(0,n.jsxDEV)(e.i,{children:\"Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\", 2459\\u20132468.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.div,{className:\"csl-entry\",id:\"bib-stoyanovich-howe-jagadish\",children:[\"Stoyanovich, J., Howe, B., & Jagadish, H. (2020). Responsible data management. \",(0,n.jsxDEV)(e.i,{children:\"Proceedings of the VLDB Endowment\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\", \",(0,n.jsxDEV)(e.i,{children:\"13\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"(12), 3474\\u20133488.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.div,{className:\"csl-entry\",id:\"bib-vallor2016\",children:[\"Vallor, S. (2016). \",(0,n.jsxDEV)(e.i,{children:\"Technology and the virtues: A philosophical guide to a future worth wanting\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\". Oxford University Press.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.div,{className:\"csl-entry\",id:\"bib-verma-rubin\",children:[\"Verma, S., & Rubin, J. (2018). Fairness definitions explained. \",(0,n.jsxDEV)(e.i,{children:\"2018 Ieee/Acm International Workshop on Software Fairness (Fairware)\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\", 1\\u20137.\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.section,{className:\"footnotes\",\"data-footnotes\":!0,children:[(0,n.jsxDEV)(e.h2,{id:\"footnote-label\",className:\"sr-only\",children:[(0,n.jsxDEV)(e.a,{href:\"#footnote-label\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,n.jsxDEV)(e.span,{className:\"icon icon-link\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\"Footnotes\"]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),(0,n.jsxDEV)(e.ol,{children:[(0,n.jsxDEV)(e.li,{id:\"user-content-fn-1\",children:(0,n.jsxDEV)(e.p,{children:[\"For more information, please refer to the \",(0,n.jsxDEV)(e.a,{href:\"https://www.mas.gov.sg/news/media-releases/2021/mas-launches-global-challenge-to-accelerate-innovation-in-responsible-ai-solutions\",children:\"challenge press release\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:19,columnNumber:286},this),\" and the \",(0,n.jsxDEV)(e.a,{href:\"https://www.mas.gov.sg/-/media/MAS/News/Media-Releases/2021/Veritas-Document-1-FEAT-Fairness-Principles-Assessment-Methodology.pdf\",children:\"recommended assessment methodology\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:19,columnNumber:452},this),\". \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-1\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:19,columnNumber:242},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-2\",children:(0,n.jsxDEV)(e.p,{children:[\"There are other open-source solutions as well and I will address the differences in the next post, but this post should give a flavour of what we are going to do differently. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-2\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:25,columnNumber:193},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-3\",children:(0,n.jsxDEV)(e.p,{children:[\"e.g. accuracy or area under ROC curve for a binary classifier or root mean square error for a continuous outcome \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-3\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:37,columnNumber:276},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-4\",children:(0,n.jsxDEV)(e.p,{children:[\"The word bias in this post should be understood in the social justice sense and not as the statistical concept \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-4\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:67,columnNumber:55},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-5\",children:(0,n.jsxDEV)(e.p,{children:[\"Discriminatory is used over here in the sense of being able to differentiate, not in the social or legal sense of the word. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-5\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:79,columnNumber:31},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-6\",children:(0,n.jsxDEV)(e.p,{children:[\"I think there's no lack of a human's ability to discriminate as well, we do it every day. There just seems to be a fear when it comes to making such discriminations explicit. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-6\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:87,columnNumber:124},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-7\",children:(0,n.jsxDEV)(e.p,{children:['Could something be fair but unethical or unfair but ethical? It lies very much in the definition of the terms, but here are examples of each. One could argue that as a system for justice \"an eye for an eye\" is fair but unethical. On the other hand, treating people with different vaccination statuses differently would probably be unfair but ethical. ',(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-7\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:93,columnNumber:118},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-8\",children:(0,n.jsxDEV)(e.p,{children:[\"In the case of a binary classifier, we can assign a score to each potential outcome (true positive, false positive, true negative and false negative) and use the scores as part of the objective function. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-8\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:99,columnNumber:352},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-9\",children:(0,n.jsxDEV)(e.p,{children:[\"Differing definitions on what a good will is gives us different Kantian schools of thought. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-9\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:111,columnNumber:245},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-10\",children:(0,n.jsxDEV)(e.p,{children:[\"It might be morally praiseworthy but that's no categorical ought. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-10\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:113,columnNumber:152},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-11\",children:(0,n.jsxDEV)(e.p,{children:[\"The list of values proposed by Vallor could be a good start: honesty, self-control, humility, justice, courage, empathy, care, civility, flexibility, perspective, magnanimity, and technomoral wisdom. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-11\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:127,columnNumber:601},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-12\",children:(0,n.jsxDEV)(e.p,{children:[\"In a utilitarian framework, fairness could simply arise if we assume individuals have a utility function which exhibits diminishing marginal utility. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-12\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:139,columnNumber:307},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-13\",children:(0,n.jsxDEV)(e.p,{children:[\"E.g. Affirmative action policies and wealth redistribution policies are examples of how a society might try to mitigate existing unfair biases. To what extent are the policies sufficient to achieve equity is another question. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-13\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:149,columnNumber:539},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-14\",children:(0,n.jsxDEV)(e.p,{children:[\"An exception is the counterfactual school of thought which requires constructing a causal graph which describes the relationships between particular variables, observed or unobserved, and mapping a definition of fairness based on hypothesised pathways and relations. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-14\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:153,columnNumber:417},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-15\",children:(0,n.jsxDEV)(e.p,{children:[\"See \",(0,n.jsxDEV)(e.span,{id:\"citation--barocas-hardt-narayanan--13\",children:(0,n.jsxDEV)(e.a,{href:\"#bib-barocas-hardt-narayanan\",children:\"Barocas et al. (2019)\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this),\" for a proof of independence versus separation. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-15\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:157,columnNumber:370},this),(0,n.jsxDEV)(e.li,{id:\"user-content-fn-16\",children:(0,n.jsxDEV)(e.p,{children:[\"Note, this refers to one outcome, not 20 different fairness outcomes. \",(0,n.jsxDEV)(e.a,{href:\"#user-content-fnref-16\",\"aria-label\":\"Back to content\",className:\"data-footnote-backref\",\"data-footnote-backref\":!0,children:\"\\u21A9\"},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)},void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:157,columnNumber:631},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)]},void 0,!0,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\",lineNumber:1,columnNumber:1},this)}function gn(l={}){let{wrapper:e}=l.components||{};return e?(0,n.jsxDEV)(e,Object.assign({},l,{children:(0,n.jsxDEV)(Ie,l,void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this)}),void 0,!1,{fileName:\"/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx\"},this):Ie(l)}var xn=gn;function Ee(l,e,m){throw new Error(\"Expected \"+(e?\"component\":\"object\")+\" `\"+l+\"` to be defined: you likely forgot to import, pass, or provide it.\"+(m?\"\\nIt\\u2019s referenced in your code at `\"+m+\"` in `/Users/canhnd/Desktop/code/2024/myblog/_mdx_bundler_entry_point-508e39ad-db2c-4be2-ba62-fd9948cb432a.mdx`\":\"\"))}return _n(Nn);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Facebook, Inc. and its affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "blog/2021-09-10-a-human-centric-approach-to-fairness-in-ai.mdx",
  "_raw": {
    "sourceFilePath": "blog/2021-09-10-a-human-centric-approach-to-fairness-in-ai.mdx",
    "sourceFileName": "2021-09-10-a-human-centric-approach-to-fairness-in-ai.mdx",
    "sourceFileDir": "blog",
    "contentType": "mdx",
    "flattenedPath": "blog/2021-09-10-a-human-centric-approach-to-fairness-in-ai"
  },
  "type": "Blog",
  "readingTime": {
    "text": "21 min read",
    "minutes": 20.06,
    "time": 1203600,
    "words": 4012
  },
  "slug": "a-human-centric-approach-to-fairness-in-ai",
  "path": "blog/a-human-centric-approach-to-fairness-in-ai",
  "filePath": "blog/2021-09-10-a-human-centric-approach-to-fairness-in-ai.mdx",
  "toc": [
    {
      "value": "Background",
      "url": "#background",
      "depth": 2
    },
    {
      "value": "The case for a single fairness metric",
      "url": "#the-case-for-a-single-fairness-metric",
      "depth": 2
    },
    {
      "value": "A brief survey of fairness metrics",
      "url": "#a-brief-survey-of-fairness-metrics",
      "depth": 2
    },
    {
      "value": "Data as mirrors",
      "url": "#data-as-mirrors",
      "depth": 2
    },
    {
      "value": "Models as discriminatory",
      "url": "#models-as-discriminatory",
      "depth": 2
    },
    {
      "value": "Ethics and Fairness",
      "url": "#ethics-and-fairness",
      "depth": 2
    },
    {
      "value": "Utilitarianism",
      "url": "#utilitarianism",
      "depth": 3
    },
    {
      "value": "Deontological Ethics",
      "url": "#deontological-ethics",
      "depth": 3
    },
    {
      "value": "Virtues",
      "url": "#virtues",
      "depth": 3
    },
    {
      "value": "Practical Ethics",
      "url": "#practical-ethics",
      "depth": 3
    },
    {
      "value": "Fairness",
      "url": "#fairness",
      "depth": 3
    },
    {
      "value": "Fairness in AI systems",
      "url": "#fairness-in-ai-systems",
      "depth": 2
    },
    {
      "value": "Conclusion",
      "url": "#conclusion",
      "depth": 2
    },
    {
      "value": "References",
      "url": "#references",
      "depth": 2
    }
  ],
  "structuredData": {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "A Human-centric Approach to Fairness in AI",
    "datePublished": "2021-09-16T00:00:00.000Z",
    "dateModified": "2021-12-24T00:00:00.000Z",
    "description": "Fairness is messy and complicated. Attempts to distil it down to a single metric is unhelpful and counter-productive. As business owners and model developers we should embrace the struggle in trying to apply fairness in artificial intelligence and data analytics models.",
    "image": "/static/img/fairness/human-centric-approach-to-ai-cover.png",
    "url": "https://www.davidnguyen.blog/a-human-centric-approach-to-fairness-in-ai"
  }
}